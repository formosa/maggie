# Maggie AI Assistant Configuration
# Optimized for AMD Ryzen 9 5900X and NVIDIA GeForce RTX 3080
# System configuration for performance and resource management

# System configuration
inactivity_timeout: 60  # minutes in seconds

# STT configuration
stt:
  # Whisper STT configuration
  whisper:
    # Options: "base", "large"
    model_size: "base"
    # Use float16 for faster inference on RTX 3080
    compute_type: "float16" 
    # Path to Whisper model directory
    model_path: "\\maggie\\models\\stt\\whisper"
    # Sample rate for audio input 
    sample_rate: 16000 
    # Hardware acceleration flags
    tensor_cores_enabled: true
    flash_attention_enabled: true
    max_batch_size: 16
    memory_efficient: true
    # Processing optimizations
    parallel_processing: true
    chunk_size: 512  # Optimized for Ryzen 9 5900X
    simd_optimization: true
    cache_models: true
  # Whisper streaming configuration
  whisper_streaming:
    # Enable streaming mode for real-time transcription
    enabled: true
    # Model name (same as whisper model_size if not specified)
    model_name: "base"
    # Language code
    language: "en"
    # Compute type (float16 for RTX 3080)
    compute_type: "float16"
    # Seconds to wait for intermediate results
    result_timeout: 0.5
    # Seconds of silence before finalizing result
    silence_threshold: 2.0
    # Whether to auto-submit final results
    auto_submit: false
    # Buffer size in seconds
    buffer_size_seconds: 30.0
    # Hardware-specific optimizations
    cuda_streams: 2
    batch_processing: true
    low_latency_mode: true
    tensor_cores_enabled: true
    dedicated_threads: 2
    thread_affinity_enabled: true
  # Wake word detection configuration
  wake_word:
    # Options: "porcupine", "snowboy"
    engine: "porcupine"
    # Porcupine access key
    access_key: "9hQNkT59LV9YLj02ilmzaIg4zqUjX0YycaHTcHPEJuQ1/58G/r4BXA=="
    # Sensitivity for wake word detection
    sensitivity: 0.5
    # Porcupine wake word configuration
    keyword: "maggie"
    # Custom keyword file path (optional)
    keyword_path: "\\maggie\\models\\stt\\porcupine\\Hey-Maggie_en_windows_v3_0_0.ppn"
    # Maximum CPU usage percentage for wake word detection
    cpu_threshold: 5.0
    # CPU optimization
    dedicated_core_enabled: true
    dedicated_core: 0  # Use core 0 for wake word detection
    real_time_priority: true
    minimal_processing: true

# TTS configuration 
tts:
  voice_model: "af_heart.pt"
  model_path: "\\maggie\\models\\tts"
  sample_rate: 22050
  use_cache: true
  cache_dir: "\\maggie\\cache\\tts"
  cache_size: 200
  gpu_device: 0
  gpu_acceleration: true
  gpu_precision: "mixed_float16"
  max_workers: 4
  voice_preprocessing: true
  # Hardware acceleration settings
  tensor_cores_enabled: true
  cuda_graphs_enabled: true
  amp_optimization_level: "O2"  # Automatic mixed precision optimization level
  max_batch_size: 64
  dynamic_memory_management: true
  # Processing parameters optimized for Ryzen 9 5900X
  dedicated_threads: 2
  thread_affinity_enabled: true
  thread_affinity_cores: [8, 9]  # Use dedicated cores 8-9 for TTS
  realtime_priority: true
  simd_optimization: true  # Use SIMD instructions
  # Audio processing parameters
  buffer_size: 4096
  spectral_processing_on_gpu: true  # Use GPU for spectral processing

# LLM configuration
llm:
  model_path: "maggie\\models\\llm\\mistral-7b-instruct-v0.3-GPTQ-4bit"
  model_type: "mistral"
  gpu_layers: 32  # Optimized for RTX 3080 10GB
  gpu_layer_auto_adjust: true  # Allow automatic adjustment based on available VRAM
  # Optimization flags
  tensor_cores_enabled: true  # Use tensor cores for matrix operations
  mixed_precision_enabled: true  # Enable mixed precision training/inference
  precision_type: "float16"  # Use FP16 precision
  kv_cache_optimization: true  # Optimize key-value cache usage
  attention_optimization: true  # Enable optimized attention mechanism
  # Memory optimizations
  context_length: 8192  # Maximum context length
  batch_size: 16  # Batch size for inference
  offload_strategy: "auto"  # Automatically decide what to offload to CPU
  # RTX 3080 specific settings
  vram_efficient_loading: true
  rtx_3080_optimized: true

# Logging configuration
logging:
  # Path to log directory
  path: "logs"
  # Log levels for console and file logging: DEBUG, INFO, WARNING, ERROR
  console_level: "INFO"
  # Separate level for file logging
  file_level: "DEBUG"
  # Batch size for asynchronous logging
  batch_size: 50
  # Timeout for batched logging in seconds
  batch_timeout: 5.0
  # Enable/disable asynchronous logging
  async_enabled: true

# Extensions configuration
extensions:
  # Enable or disable specific extension modules
  recipe_creator:
    # Enable or disable the recipe creator module
    enabled: true
    # Path to the recipe template file
    template_path: "templates\\recipe_template.docx"
    # Output directory for generated recipes
    output_dir: "recipes"

# CPU configuration
cpu:
  # Number of worker threads for parallel processing
  # (using 8 of the 12 available cores for Ryzen 9 5900X)
  max_threads: 8
  # Timeout for thread operations in seconds
  thread_timeout: 30  
  # Ryzen 9 5900X specific optimizations
  ryzen_9_5900x_optimized: true
  thread_affinity_enabled: true
  # Prioritize performance cores (first 8 cores)
  performance_cores: [0, 1, 2, 3, 4, 5, 6, 7]
  background_cores: [8, 9, 10, 11]
  # Power management settings
  high_performance_plan: true
  disable_core_parking: true
  # AMD-specific optimizations
  precision_boost_overdrive: true
  simultaneous_multithreading: true

# Memory configuration
memory:
  # Maximum memory usage for the application as a percentage of total system memory
  # (using up to 75% of system memory (24GB) from 32GB system)
  max_percent: 75
  # Threshold for unloading models to free up memory
  # (unload models if memory usage exceeds 85%)
  model_unload_threshold: 85
  # XPG D10 DDR4-3200 specific settings
  xpg_d10_memory: true
  # Use large pages for better memory performance
  large_pages_enabled: true
  # Enable NUMA-aware memory allocation for Ryzen 9 5900X
  numa_aware: true
  # Memory allocation settings
  preload_models: true
  cache_size_mb: 6144  # 6GB cache size for 32GB system
  min_free_gb: 4  # Maintain at least 4GB free RAM
  defragmentation_threshold: 70  # Percentage threshold for memory defragmentation

# GPU configuration
gpu:
  # Maximum GPU memory usage as a percentage of total GPU memory
  # (using up to 90% of RTX 3080 (10 GB VRAM) memory)
  max_percent: 90
  # Threshold for unloading models to free up memory
  # (unload models if memory usage exceeds 95%)
  model_unload_threshold: 95
  # RTX 3080 specific optimizations
  rtx_3080_optimized: true
  tensor_cores_enabled: true
  tensor_precision: "tf32"  # Use TF32 precision for tensor operations
  # CUDA settings
  cuda_compute_type: "float16"  # Use FP16 for compute operations
  cuda_streams: 3  # Number of CUDA streams for parallel execution
  cuda_memory_pool: true  # Enable CUDA memory pooling
  cuda_graphs: true  # Enable CUDA graphs for optimized execution
  # VRAM management settings
  max_batch_size: 16
  reserved_memory_mb: 256
  dynamic_memory: true
  fragmentation_threshold: 15  # Percentage threshold for memory defragmentation
  pre_allocation: true